{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color: #004B87; padding: 20px; text-align: center; border-radius: 10px; color: white; font-family: Arial, sans-serif; margin: auto; width: 80%;\">\n",
        "    <h1>Module 3 Assignment</h1>\n",
        "    <h2>RURANGWA IRADUKUNDA Jean-François Régis</h2>\n",
        "    <h3>September 7, 2025</h3>\n",
        "    <p>Email : jeanfrancoisregis.rurangwairadukunda@axa.be</p>\n",
        "</div>"
      ],
      "metadata": {
        "id": "xcKz6uU8Ce7X"
      },
      "id": "xcKz6uU8Ce7X"
    },
    {
      "cell_type": "markdown",
      "id": "516ee1d7-d4a0-4378-9a73-ff977fe061e6",
      "metadata": {
        "id": "516ee1d7-d4a0-4378-9a73-ff977fe061e6"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "\n",
        "Following the 'Actuarial Data Scientist' program offered by the Belgian association of actuaries (IABE), we are asked with an assignment that builds on the foundational concepts learned in the third and last module.\n",
        "<br/>\n",
        "This assignment involves analyzing the \"eusavingULnoPS\" dataset which is based on unit-linked saving products, with no profit sharing, sold in an unknown European country. Those insurance policies are observed between 1999 and 2008, with entries and exits possible.\n",
        "<br/>\n",
        "\n",
        "In this analysis, we will focus on developing classification and regression models for specific targets, applying feature selection techniques, and implementing explainability and fairness assessments.\n",
        "<br/>\n",
        "The objective is to provide actionable insights into the factors influencing policy lapses and risk premium proportions, ensuring the models are transparent, fair, and robust."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EUzqdV0xOgZZ",
      "metadata": {
        "id": "EUzqdV0xOgZZ"
      },
      "source": [
        "## Importing packages <a name=\"Importing_packages\"></a>\n",
        "\n",
        "Our first step is to import and load the packages that will be necessary in our task.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YuP6mnV0_mJi",
      "metadata": {
        "id": "YuP6mnV0_mJi",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "## Perform these pip installs in the command prompt, with your python kernel activated\n",
        "!pip install shap\n",
        "!pip install tensorflow\n",
        "!pip install shapicant\n",
        "!pip install scikeras\n",
        "!pip install mlflow\n",
        "!pip install evidently\n",
        "!pip install boruta\n",
        "!pip install pandas scikit-learn deap\n",
        "!pip install aif360\n",
        "\n",
        "!git clone https://github.com/Regis0323/Module-3-assignment.git\n",
        "!pip install scikit-fuzzy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b658fa8-eb1f-42b4-8178-42687d0f6f95",
      "metadata": {
        "id": "9b658fa8-eb1f-42b4-8178-42687d0f6f95",
        "tags": [],
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# 1. SETUP: load libraries & data\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Data preprocesing and models\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV\n",
        "from sklearn.linear_model import LassoCV\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "!pip install tensorflow\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "# MLFlow\n",
        "import mlflow\n",
        "import cloudpickle\n",
        "from evidently import Dataset\n",
        "from evidently import DataDefinition\n",
        "from evidently import Report\n",
        "from evidently.presets import DataDriftPreset, DataSummaryPreset\n",
        "\n",
        "# Feature selection and explainability of the model\n",
        "from mlxtend.feature_selection import SequentialFeatureSelector\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.inspection import permutation_importance\n",
        "from boruta import BorutaPy\n",
        "from deap import base, creator, tools, algorithms\n",
        "import random\n",
        "\n",
        "# Bias mitigation\n",
        "from aif360.datasets import StandardDataset, BinaryLabelDataset\n",
        "from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric\n",
        "from aif360.algorithms.preprocessing.optim_preproc_helpers.opt_tools import OptTools\n",
        "from aif360.algorithms.preprocessing import Reweighing, OptimPreproc\n",
        "from aif360.algorithms.inprocessing import MetaFairClassifier, AdversarialDebiasing\n",
        "from aif360.algorithms.postprocessing import RejectOptionClassification, EqOddsPostprocessing\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2W8hGUD_Pb7q",
      "metadata": {
        "id": "2W8hGUD_Pb7q"
      },
      "source": [
        "# Data exploration and preoprocessing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we load the dataset and get its specificities."
      ],
      "metadata": {
        "id": "Fix3fVKMTmvF"
      },
      "id": "Fix3fVKMTmvF"
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "3f6e45e6-3c45-4ceb-8db7-130c4a5f9d2d",
      "metadata": {
        "id": "3f6e45e6-3c45-4ceb-8db7-130c4a5f9d2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 325
        },
        "outputId": "a5a29180-7aa9-4d57-bdf6-6ffbfdcd5369"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  policy.ID  issue.date termination.date lapse.reason premium.frequency  \\\n",
              "0        N1  1999-01-01       2006-04-01        Claim            unique   \n",
              "1        N2  1999-01-01       2006-04-01        Claim            unique   \n",
              "2        N3  1999-01-01       2005-04-01    Surrender            unique   \n",
              "3        N4  1999-01-01       2003-02-01    Surrender            unique   \n",
              "4        N5  1999-01-01       2008-01-01     In force            unique   \n",
              "\n",
              "   gender  underwriting.age  face.amount  risk.premium  saving.premium  ...  \\\n",
              "0    Male                77      2979.53        265.86        29795.27  ...   \n",
              "1    Male                77      3039.71        271.23        30397.16  ...   \n",
              "2  Female                72      2994.12         80.43        35929.44  ...   \n",
              "3    Male                35      3051.10          7.50        30511.01  ...   \n",
              "4    Male                40      5810.21         81.39        58102.11  ...   \n",
              "\n",
              "   rate2Y.relvar1mth  rate2Y.relvar1qtr  rate10Y.relvar1mth  \\\n",
              "0           0.086310           0.184211            0.081598   \n",
              "1           0.086310           0.184211            0.081598   \n",
              "2          -0.025391          -0.018489           -0.023343   \n",
              "3          -0.041762          -0.133838           -0.038309   \n",
              "4           0.037712          -0.004430            0.025938   \n",
              "\n",
              "   rate10Y.relvar1qtr  unemploy.relvar1mth  unemploy.relvar1qtr  \\\n",
              "0            0.144626            -0.011494            -0.011494   \n",
              "1            0.144626            -0.011494            -0.011494   \n",
              "2           -0.018868            -0.010309            -0.040816   \n",
              "3           -0.039273             0.000000            -0.008850   \n",
              "4           -0.002027             0.022727             0.069767   \n",
              "\n",
              "   industry.relvar1mth  industry.relvar1qtr  RTV.relvar1mth  RTV.relvar1qtr  \n",
              "0            -0.000979             0.016699        0.005941        0.006931  \n",
              "1            -0.000979             0.016699        0.005941        0.006931  \n",
              "2             0.001000             0.003024        0.002002        0.000998  \n",
              "3            -0.003067             0.000000        0.009524        0.009574  \n",
              "4             0.001876             0.000941       -0.011550       -0.026692  \n",
              "\n",
              "[5 rows x 30 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-99b646ce-5683-40d5-a922-b87807fc55cb\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>policy.ID</th>\n",
              "      <th>issue.date</th>\n",
              "      <th>termination.date</th>\n",
              "      <th>lapse.reason</th>\n",
              "      <th>premium.frequency</th>\n",
              "      <th>gender</th>\n",
              "      <th>underwriting.age</th>\n",
              "      <th>face.amount</th>\n",
              "      <th>risk.premium</th>\n",
              "      <th>saving.premium</th>\n",
              "      <th>...</th>\n",
              "      <th>rate2Y.relvar1mth</th>\n",
              "      <th>rate2Y.relvar1qtr</th>\n",
              "      <th>rate10Y.relvar1mth</th>\n",
              "      <th>rate10Y.relvar1qtr</th>\n",
              "      <th>unemploy.relvar1mth</th>\n",
              "      <th>unemploy.relvar1qtr</th>\n",
              "      <th>industry.relvar1mth</th>\n",
              "      <th>industry.relvar1qtr</th>\n",
              "      <th>RTV.relvar1mth</th>\n",
              "      <th>RTV.relvar1qtr</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>N1</td>\n",
              "      <td>1999-01-01</td>\n",
              "      <td>2006-04-01</td>\n",
              "      <td>Claim</td>\n",
              "      <td>unique</td>\n",
              "      <td>Male</td>\n",
              "      <td>77</td>\n",
              "      <td>2979.53</td>\n",
              "      <td>265.86</td>\n",
              "      <td>29795.27</td>\n",
              "      <td>...</td>\n",
              "      <td>0.086310</td>\n",
              "      <td>0.184211</td>\n",
              "      <td>0.081598</td>\n",
              "      <td>0.144626</td>\n",
              "      <td>-0.011494</td>\n",
              "      <td>-0.011494</td>\n",
              "      <td>-0.000979</td>\n",
              "      <td>0.016699</td>\n",
              "      <td>0.005941</td>\n",
              "      <td>0.006931</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>N2</td>\n",
              "      <td>1999-01-01</td>\n",
              "      <td>2006-04-01</td>\n",
              "      <td>Claim</td>\n",
              "      <td>unique</td>\n",
              "      <td>Male</td>\n",
              "      <td>77</td>\n",
              "      <td>3039.71</td>\n",
              "      <td>271.23</td>\n",
              "      <td>30397.16</td>\n",
              "      <td>...</td>\n",
              "      <td>0.086310</td>\n",
              "      <td>0.184211</td>\n",
              "      <td>0.081598</td>\n",
              "      <td>0.144626</td>\n",
              "      <td>-0.011494</td>\n",
              "      <td>-0.011494</td>\n",
              "      <td>-0.000979</td>\n",
              "      <td>0.016699</td>\n",
              "      <td>0.005941</td>\n",
              "      <td>0.006931</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>N3</td>\n",
              "      <td>1999-01-01</td>\n",
              "      <td>2005-04-01</td>\n",
              "      <td>Surrender</td>\n",
              "      <td>unique</td>\n",
              "      <td>Female</td>\n",
              "      <td>72</td>\n",
              "      <td>2994.12</td>\n",
              "      <td>80.43</td>\n",
              "      <td>35929.44</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.025391</td>\n",
              "      <td>-0.018489</td>\n",
              "      <td>-0.023343</td>\n",
              "      <td>-0.018868</td>\n",
              "      <td>-0.010309</td>\n",
              "      <td>-0.040816</td>\n",
              "      <td>0.001000</td>\n",
              "      <td>0.003024</td>\n",
              "      <td>0.002002</td>\n",
              "      <td>0.000998</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>N4</td>\n",
              "      <td>1999-01-01</td>\n",
              "      <td>2003-02-01</td>\n",
              "      <td>Surrender</td>\n",
              "      <td>unique</td>\n",
              "      <td>Male</td>\n",
              "      <td>35</td>\n",
              "      <td>3051.10</td>\n",
              "      <td>7.50</td>\n",
              "      <td>30511.01</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.041762</td>\n",
              "      <td>-0.133838</td>\n",
              "      <td>-0.038309</td>\n",
              "      <td>-0.039273</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.008850</td>\n",
              "      <td>-0.003067</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.009524</td>\n",
              "      <td>0.009574</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>N5</td>\n",
              "      <td>1999-01-01</td>\n",
              "      <td>2008-01-01</td>\n",
              "      <td>In force</td>\n",
              "      <td>unique</td>\n",
              "      <td>Male</td>\n",
              "      <td>40</td>\n",
              "      <td>5810.21</td>\n",
              "      <td>81.39</td>\n",
              "      <td>58102.11</td>\n",
              "      <td>...</td>\n",
              "      <td>0.037712</td>\n",
              "      <td>-0.004430</td>\n",
              "      <td>0.025938</td>\n",
              "      <td>-0.002027</td>\n",
              "      <td>0.022727</td>\n",
              "      <td>0.069767</td>\n",
              "      <td>0.001876</td>\n",
              "      <td>0.000941</td>\n",
              "      <td>-0.011550</td>\n",
              "      <td>-0.026692</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 30 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-99b646ce-5683-40d5-a922-b87807fc55cb')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-99b646ce-5683-40d5-a922-b87807fc55cb button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-99b646ce-5683-40d5-a922-b87807fc55cb');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-89ea3345-f874-46c8-9093-566d10b13c1e\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-89ea3345-f874-46c8-9093-566d10b13c1e')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-89ea3345-f874-46c8-9093-566d10b13c1e button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "float64    23\n",
            "object      6\n",
            "int64       1\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Load\n",
        "df = pd.read_csv(\"/content/Module-3-assignment/eusavingULnoPS.csv\")\n",
        "display(df.head())\n",
        "print(df.dtypes.value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "k9dGsrG-axlE",
      "metadata": {
        "id": "k9dGsrG-axlE"
      },
      "source": [
        "Our dataset is composed of 30 columns, of which 6 are categorized as 'object', 1 as 64-bit 'integer' (int64) and 23 as 64-bit 'floating-point' (float64) numbers.\n",
        "<br>\n",
        "We shall now check for missing and/or duplicated values before treating them accordingly if any."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "r24UfL7G7M7e",
      "metadata": {
        "id": "r24UfL7G7M7e",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1f2e909-4def-493c-e2ce-e23ff55afcd2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "policy.ID              0\n",
            "issue.date             0\n",
            "termination.date       0\n",
            "lapse.reason           0\n",
            "premium.frequency      0\n",
            "gender                 0\n",
            "underwriting.age       0\n",
            "face.amount            0\n",
            "risk.premium           0\n",
            "saving.premium         0\n",
            "CPI.relvar1mth         0\n",
            "CPI.relvar1qtr         0\n",
            "CPI.relvar1yr          0\n",
            "CPI.relvar2yr          0\n",
            "EUidx.relvar1mth       0\n",
            "EUidx.relvar1qtr       0\n",
            "EUidx.relvar1yr        0\n",
            "EUidx.relvar2yr        0\n",
            "rate1Y.relvar1mth      0\n",
            "rate1Y.relvar1qtr      0\n",
            "rate2Y.relvar1mth      0\n",
            "rate2Y.relvar1qtr      0\n",
            "rate10Y.relvar1mth     0\n",
            "rate10Y.relvar1qtr     0\n",
            "unemploy.relvar1mth    0\n",
            "unemploy.relvar1qtr    0\n",
            "industry.relvar1mth    0\n",
            "industry.relvar1qtr    0\n",
            "RTV.relvar1mth         0\n",
            "RTV.relvar1qtr         0\n",
            "dtype: int64\n",
            "Empty DataFrame\n",
            "Columns: [policy.ID, issue.date, termination.date, lapse.reason, premium.frequency, gender, underwriting.age, face.amount, risk.premium, saving.premium, CPI.relvar1mth, CPI.relvar1qtr, CPI.relvar1yr, CPI.relvar2yr, EUidx.relvar1mth, EUidx.relvar1qtr, EUidx.relvar1yr, EUidx.relvar2yr, rate1Y.relvar1mth, rate1Y.relvar1qtr, rate2Y.relvar1mth, rate2Y.relvar1qtr, rate10Y.relvar1mth, rate10Y.relvar1qtr, unemploy.relvar1mth, unemploy.relvar1qtr, industry.relvar1mth, industry.relvar1qtr, RTV.relvar1mth, RTV.relvar1qtr]\n",
            "Index: []\n",
            "\n",
            "[0 rows x 30 columns]\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 21921 entries, 0 to 21920\n",
            "Data columns (total 30 columns):\n",
            " #   Column               Non-Null Count  Dtype  \n",
            "---  ------               --------------  -----  \n",
            " 0   policy.ID            21921 non-null  object \n",
            " 1   issue.date           21921 non-null  object \n",
            " 2   termination.date     21921 non-null  object \n",
            " 3   lapse.reason         21921 non-null  object \n",
            " 4   premium.frequency    21921 non-null  object \n",
            " 5   gender               21921 non-null  object \n",
            " 6   underwriting.age     21921 non-null  int64  \n",
            " 7   face.amount          21921 non-null  float64\n",
            " 8   risk.premium         21921 non-null  float64\n",
            " 9   saving.premium       21921 non-null  float64\n",
            " 10  CPI.relvar1mth       21921 non-null  float64\n",
            " 11  CPI.relvar1qtr       21921 non-null  float64\n",
            " 12  CPI.relvar1yr        21921 non-null  float64\n",
            " 13  CPI.relvar2yr        21921 non-null  float64\n",
            " 14  EUidx.relvar1mth     21921 non-null  float64\n",
            " 15  EUidx.relvar1qtr     21921 non-null  float64\n",
            " 16  EUidx.relvar1yr      21921 non-null  float64\n",
            " 17  EUidx.relvar2yr      21921 non-null  float64\n",
            " 18  rate1Y.relvar1mth    21921 non-null  float64\n",
            " 19  rate1Y.relvar1qtr    21921 non-null  float64\n",
            " 20  rate2Y.relvar1mth    21921 non-null  float64\n",
            " 21  rate2Y.relvar1qtr    21921 non-null  float64\n",
            " 22  rate10Y.relvar1mth   21921 non-null  float64\n",
            " 23  rate10Y.relvar1qtr   21921 non-null  float64\n",
            " 24  unemploy.relvar1mth  21921 non-null  float64\n",
            " 25  unemploy.relvar1qtr  21921 non-null  float64\n",
            " 26  industry.relvar1mth  21921 non-null  float64\n",
            " 27  industry.relvar1qtr  21921 non-null  float64\n",
            " 28  RTV.relvar1mth       21921 non-null  float64\n",
            " 29  RTV.relvar1qtr       21921 non-null  float64\n",
            "dtypes: float64(23), int64(1), object(6)\n",
            "memory usage: 5.0+ MB\n",
            "['policy.ID', 'issue.date', 'termination.date', 'lapse.reason', 'premium.frequency', 'gender', 'underwriting.age', 'face.amount', 'risk.premium', 'saving.premium', 'CPI.relvar1mth', 'CPI.relvar1qtr', 'CPI.relvar1yr', 'CPI.relvar2yr', 'EUidx.relvar1mth', 'EUidx.relvar1qtr', 'EUidx.relvar1yr', 'EUidx.relvar2yr', 'rate1Y.relvar1mth', 'rate1Y.relvar1qtr', 'rate2Y.relvar1mth', 'rate2Y.relvar1qtr', 'rate10Y.relvar1mth', 'rate10Y.relvar1qtr', 'unemploy.relvar1mth', 'unemploy.relvar1qtr', 'industry.relvar1mth', 'industry.relvar1qtr', 'RTV.relvar1mth', 'RTV.relvar1qtr']\n"
          ]
        }
      ],
      "source": [
        "#Potential missing values\n",
        "Missing_Values=df.isna().sum()\n",
        "print(Missing_Values)\n",
        "\n",
        "#Potential duplicated values\n",
        "Duplicated_Values=df[df.duplicated(keep=False)]\n",
        "print(Duplicated_Values)\n",
        "  #Treating duplicated values (if any)\n",
        "df_cleaned=df.drop_duplicates(keep='first')\n",
        "\n",
        "#Information about the new cleaned dataset\n",
        "df_cleaned.info()\n",
        "print(df_cleaned.columns.tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "099I_dd4bD61",
      "metadata": {
        "id": "099I_dd4bD61"
      },
      "source": [
        "After this check, we can see that there are neither missing values nor duplicates and there are 2 date variables that have been properly treated.\n",
        "\n",
        "We shall now create two datasets (df_cat and df_num), one for each of the following two dependent variables:\n",
        "*   a categorical one: lapse.reason;\n",
        "*   a numeric one: a numerical variable that represents the proportion of risk premium over total premium: proportion_risk_premium = risk.premium / (risk.premium+saving.premium)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "xlLmk_Rsx7vc",
      "metadata": {
        "id": "xlLmk_Rsx7vc"
      },
      "outputs": [],
      "source": [
        "# Conversion of the date columns from 'object' to 'datetime'\n",
        "df_cleaned['issue.date'] = pd.to_datetime(df['issue.date'], errors='coerce')\n",
        "df_cleaned['termination.date'] = pd.to_datetime(df['termination.date'], errors='coerce')\n",
        "\n",
        "# Creation of numerical and categorical datasets\n",
        "df_cat = df_cleaned.copy()\n",
        "df_num = df_cleaned.copy()\n",
        "\n",
        "# Creation of numerical dependent variable\n",
        "df_num['proportion_risk_premium'] = df_num['risk.premium']/(df_num['risk.premium'] + df_num['saving.premium'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XZ5Mhh5DbJ9c",
      "metadata": {
        "id": "XZ5Mhh5DbJ9c"
      },
      "source": [
        "## Data preprocessing\n",
        "After creating the numerical and categorical datasets, we shall now define the X (X_cat and X_num) and y datasets before converting categorical variables to a suitable format (using the \"one-hot encoding\" method), handling date variables and splitting the datasets into training and test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "kxMkhp8KQ01J",
      "metadata": {
        "id": "kxMkhp8KQ01J"
      },
      "outputs": [],
      "source": [
        "# Define X and y (wisely select variables for the X datasets)\n",
        "y_num = df_num['proportion_risk_premium']\n",
        "y_cat = df_cat['lapse.reason']\n",
        "X_num = df_num.drop(['proportion_risk_premium'], axis=1)\n",
        "X_cat = df_cat.drop(['lapse.reason'], axis=1)\n",
        "\n",
        "# One-hot encoding method for categorical variables\n",
        "categorical_cols = X_cat.select_dtypes(include=['object']).columns.tolist()\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[('onehot', OneHotEncoder(handle_unknown='ignore'), categorical_cols)],\n",
        "    remainder='passthrough')\n",
        "\n",
        "# Example on how to handle date variables\n",
        "for df_ in [X_num, X_cat]:\n",
        "    df_['issue_date_year'] = df_['issue.date'].dt.year\n",
        "    df_['issue_date_month'] = df_['issue.date'].dt.month\n",
        "    df_['termination_date_year'] = df_['termination.date'].dt.year\n",
        "    df_['termination_date_month'] = df_['termination.date'].dt.month\n",
        "    df_.drop(['issue.date', 'termination.date'], axis=1, inplace=True)\n",
        "\n",
        "# Split test and train datasets\n",
        "X_train_cat, X_test_cat, y_train_cat, y_test_cat = train_test_split(X_cat, y_cat, test_size=0.25, random_state=23, stratify=y_cat)\n",
        "\n",
        "X_train_num, X_test_num, y_train_num, y_test_num = train_test_split(X_num, y_num, test_size=0.25, random_state=23)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XI885gZbJ3Sr",
      "metadata": {
        "id": "XI885gZbJ3Sr"
      },
      "source": [
        "## Development of model for categorical variable: RandomForest\n",
        "\n",
        "In this part we shall develop a Random Forest algorithm for the categorical dependent variable \"lapse.reason\"."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the pipeline with preprocessing and classifier\n",
        "rf_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),  # Your existing OneHotEncoder + passthrough\n",
        "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=23))\n",
        "])\n",
        "\n",
        "# Fit the model on training data\n",
        "rf_pipeline.fit(X_train_cat, y_train_cat)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = rf_pipeline.predict(X_test_cat)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Classification report for 'lapse.reason':\")\n",
        "print(classification_report(y_test_cat, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezPnnDZOR33z",
        "outputId": "c75bd2f4-e4e2-4c2a-bc80-b572a7b18814"
      },
      "id": "ezPnnDZOR33z",
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification report for 'lapse.reason':\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       Claim       1.00      0.10      0.19        49\n",
            "    In force       1.00      1.00      1.00      3470\n",
            "   Surrender       0.98      1.00      0.99      1962\n",
            "\n",
            "    accuracy                           0.99      5481\n",
            "   macro avg       0.99      0.70      0.72      5481\n",
            "weighted avg       0.99      0.99      0.99      5481\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary:\n",
        "The current model is performing well overall, but poorly on the minority class.\n",
        "Tuning can help improve recall for 'Claim'.\n",
        "So, yes, you should tune and experiment further."
      ],
      "metadata": {
        "id": "9zeCBbI3TVk-"
      },
      "id": "9zeCBbI3TVk-"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "ueOE244D57fa",
      "metadata": {
        "id": "ueOE244D57fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "5d70b21d-c9cd-4d1e-de40-76a383b87a19"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'X_train_cat' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3079890609.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# 1. Encode categorical variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mcategorical_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train_cat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_dtypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'object'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m preprocessor = ColumnTransformer(\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train_cat' is not defined"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# 1. Encode categorical variables\n",
        "categorical_cols = X_train_cat.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
        "    ],\n",
        "    remainder='passthrough'\n",
        ")\n",
        "\n",
        "# Fit and transform training data\n",
        "X_train_encoded = preprocessor.fit_transform(X_train_cat)\n",
        "\n",
        "# 2. Apply SMOTE on the encoded data\n",
        "smote = SMOTE(random_state=23)\n",
        "X_resampled, y_resampled = smote.fit_resample(X_train_encoded, y_train_cat)\n",
        "\n",
        "# 3. Now, X_resampled is fully numerical and balanced\n",
        "# You can train your model directly on X_resampled, y_resampled\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure imbalanced-learn is installed\n",
        "!pip install imbalanced-learn\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, precision_recall_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- Step 1: Encode your categorical features ---\n",
        "# Use your existing 'preprocessor' for encoding\n",
        "X_train_encoded = preprocessor.fit_transform(X_train_cat)\n",
        "X_test_encoded = preprocessor.transform(X_test_cat)\n",
        "\n",
        "# --- Step 2: Apply SMOTE to balance the training data ---\n",
        "smote = SMOTE(random_state=23)\n",
        "X_resampled, y_resampled = smote.fit_resample(X_train_encoded, y_train_cat)\n",
        "\n",
        "# --- Step 3: Build a pipeline with best hyperparameters and class_weight='balanced' ---\n",
        "best_params = {\n",
        "    'classifier__n_estimators': 100,\n",
        "    'classifier__min_samples_split': 2,\n",
        "    'classifier__min_samples_leaf': 1,\n",
        "    'classifier__max_features': 'log2',\n",
        "    'classifier__max_depth': None\n",
        "}\n",
        "\n",
        "clf = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', RandomForestClassifier(**best_params, class_weight='balanced', random_state=23))\n",
        "])\n",
        "\n",
        "# --- Step 4: Train on the SMOTE-balanced data ---\n",
        "clf.fit(X_resampled, y_resampled)\n",
        "\n",
        "# --- Step 5: Get probabilities on test set ---\n",
        "y_proba = clf.predict_proba(X_test_encoded)\n",
        "\n",
        "# Find index of 'Claim'\n",
        "claim_idx = list(clf.named_steps['classifier'].classes_).index('Claim')\n",
        "claim_probs = y_proba[:, claim_idx]\n",
        "\n",
        "# --- Step 6: Plot Precision-Recall curve ---\n",
        "precision, recall, thresholds = precision_recall_curve(y_test == 'Claim', claim_probs)\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(thresholds, precision[:-1], label='Precision')\n",
        "plt.plot(thresholds, recall[:-1], label='Recall')\n",
        "plt.xlabel('Decision Threshold')\n",
        "plt.title('Precision-Recall Trade-off for Claim Class')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# --- Step 7: Select an optimal threshold (e.g., 0.05) to improve recall ---\n",
        "optimal_threshold = 0.05\n",
        "y_pred_adjusted = (claim_probs >= optimal_threshold).astype(str)\n",
        "\n",
        "# --- Step 8: Evaluate at this threshold ---\n",
        "print(f\"Classification report at threshold={optimal_threshold}:\")\n",
        "print(classification_report(y_test_cat, y_pred_adjusted))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "du6TlYyQW-mp",
        "outputId": "6200d3eb-ee98-4eef-c3c4-4879e80e0374"
      },
      "id": "du6TlYyQW-mp",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.12/dist-packages (0.14.0)\n",
            "Requirement already satisfied: numpy<3,>=1.25.2 in /usr/local/lib/python3.12/dist-packages (from imbalanced-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy<2,>=1.11.4 in /usr/local/lib/python3.12/dist-packages (from imbalanced-learn) (1.16.1)\n",
            "Requirement already satisfied: scikit-learn<2,>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from imbalanced-learn) (1.6.1)\n",
            "Requirement already satisfied: joblib<2,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from imbalanced-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from imbalanced-learn) (3.6.0)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'preprocessor' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-827607487.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# --- Step 1: Encode your categorical features ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Use your existing 'preprocessor' for encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mX_train_encoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_cat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mX_test_encoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_cat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'preprocessor' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "param_dist = {\n",
        "    'classifier__n_estimators': [50, 100, 200],\n",
        "    'classifier__max_depth': [None, 10, 20],\n",
        "    'classifier__min_samples_split': [2, 5, 10],\n",
        "    'classifier__min_samples_leaf': [1, 2, 4],\n",
        "    'classifier__max_features': ['sqrt', 'log2']\n",
        "}\n",
        "\n",
        "# Set up RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(\n",
        "    rf_pipeline, param_distributions=param_dist,\n",
        "    n_iter=20, scoring='f1_weighted', cv=5, n_jobs=-1, verbose=2, random_state=23\n",
        ")\n",
        "\n",
        "# Run hyperparameter tuning\n",
        "random_search.fit(X_train_cat, y_train_cat)\n",
        "\n",
        "# Best parameters\n",
        "print(\"Best hyperparameters:\", random_search.best_params_)\n",
        "\n",
        "# Evaluate the best model\n",
        "best_model = random_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test_cat)\n",
        "print(\"Classification report after hyperparameter tuning:\")\n",
        "print(classification_report(y_test_cat, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EhgPkMb7TtyM",
        "outputId": "10a7ba97-7e95-4c8e-c725-8a0f62b7cbe6"
      },
      "id": "EhgPkMb7TtyM",
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
            "Best hyperparameters: {'classifier__n_estimators': 100, 'classifier__min_samples_split': 2, 'classifier__min_samples_leaf': 1, 'classifier__max_features': 'log2', 'classifier__max_depth': None}\n",
            "Classification report after hyperparameter tuning:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       Claim       1.00      0.10      0.19        49\n",
            "    In force       1.00      1.00      1.00      3470\n",
            "   Surrender       0.98      1.00      0.99      1962\n",
            "\n",
            "    accuracy                           0.99      5481\n",
            "   macro avg       0.99      0.70      0.72      5481\n",
            "weighted avg       0.99      0.99      0.99      5481\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Bc1ljHyeKQCj",
      "metadata": {
        "id": "Bc1ljHyeKQCj"
      },
      "source": [
        "## Development of model for numerical variable (proportion): Sequential Dense NN\n",
        "\n",
        "\n",
        "For the numerical variable \"proportion_risk_premium\", develop a Sequential Dense Neural Network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "4_-XSP6_58ah",
      "metadata": {
        "id": "4_-XSP6_58ah",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3284f02-3490-49e9-a90c-2670bce975df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.7012e-07 - mae: 3.2976e-04\n",
            "Test MAE for risk proportion: 0.00033756138873286545\n"
          ]
        }
      ],
      "source": [
        "scaler_num = StandardScaler()\n",
        "# Normalize numerical features\n",
        "X_train_num_scaled = scaler_num.fit_transform(X_train_num)\n",
        "X_test_num_scaled = scaler_num.transform(X_test_num)\n",
        "\n",
        "# Define neural network\n",
        "model = models.Sequential([\n",
        "    layers.Dense(64, activation='relu', input_shape=(X_train_num_scaled.shape[1],)),\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dense(1, activation='linear')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "# Train with early stopping\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "model.fit(\n",
        "    X_train_num_scaled,\n",
        "    y_train_num,\n",
        "    validation_split=0.2,\n",
        "    epochs=100,\n",
        "    batch_size=32,\n",
        "    callbacks=[early_stop],\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "# Evaluate\n",
        "loss, mae = model.evaluate(X_test_num_scaled, y_test_num)\n",
        "print(f\"Test MAE for risk proportion: {mae}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8RSD2zs9TRUh",
      "metadata": {
        "id": "8RSD2zs9TRUh"
      },
      "source": [
        "# Variable selection"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FlXxBhvuFhfS",
      "metadata": {
        "id": "FlXxBhvuFhfS"
      },
      "source": [
        "## Categorical problem, feature selection\n",
        "\n",
        "\n",
        "For the categorical dependent variable \"lapse.reason\", perform Boruta feature selection using (from boruta) BortutaPy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "VlXLu31W5snq",
      "metadata": {
        "id": "VlXLu31W5snq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "53e3624f-3259-42f7-e9cc-97b547fb5c7e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Sparse data was passed for X, but dense data is required. Use '.toarray()' to convert to a dense numpy array.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-502311826.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Run Boruta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mboruta_selector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBorutaPy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrf_for_boruta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m23\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mboruta_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_cat_encoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_boruta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# Get selected feature indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/boruta/boruta_py.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \"\"\"\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweak\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/boruta/boruta_py.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;31m# check input params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/boruta/boruta_py.py\u001b[0m in \u001b[0;36m_check_params\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    606\u001b[0m         \"\"\"\n\u001b[1;32m    607\u001b[0m         \u001b[0;31m# check X and y are consistent len, X is Array and y is column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperc\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperc\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'The percentile should be between 0 and 100.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1368\u001b[0m     \u001b[0mensure_all_finite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deprecate_force_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforce_all_finite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_all_finite\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m     X = check_array(\n\u001b[0m\u001b[1;32m   1371\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m         \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0m_ensure_no_complex_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m         array = _ensure_sparse_format(\n\u001b[0m\u001b[1;32m   1015\u001b[0m             \u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_ensure_sparse_format\u001b[0;34m(sparse_container, accept_sparse, dtype, copy, ensure_all_finite, accept_large_sparse, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    611\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maccept_sparse\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m         \u001b[0mpadded_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" for \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput_name\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minput_name\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 613\u001b[0;31m         raise TypeError(\n\u001b[0m\u001b[1;32m    614\u001b[0m             \u001b[0;34mf\"Sparse data was passed{padded_input}, but dense data is required. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m             \u001b[0;34m\"Use '.toarray()' to convert to a dense numpy array.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Sparse data was passed for X, but dense data is required. Use '.toarray()' to convert to a dense numpy array."
          ]
        }
      ],
      "source": [
        "categorical_features = list(X_train_cat.select_dtypes(include=['object']).columns)\n",
        "\n",
        "# Create the preprocessor with sparse output\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
        "    ],\n",
        "    remainder='passthrough'\n",
        ")\n",
        "\n",
        "# Fit and transform (sparse output)\n",
        "X_train_cat_encoded = preprocessor.fit_transform(X_train_cat)\n",
        "\n",
        "# No need to convert to dense\n",
        "# Use BorutaPy directly with sparse matrix\n",
        "rf_for_boruta = RandomForestClassifier(n_estimators=100, random_state=23)\n",
        "\n",
        "# Encode y\n",
        "le = LabelEncoder()\n",
        "y_boruta = le.fit_transform(y_train_cat)\n",
        "\n",
        "# Run Boruta\n",
        "boruta_selector = BorutaPy(rf_for_boruta, max_iter=50, random_state=23)\n",
        "boruta_selector.fit(X_train_cat_encoded, y_boruta)\n",
        "\n",
        "# Get selected feature indices\n",
        "support_mask = boruta_selector.support_\n",
        "\n",
        "# Get feature names from the OneHotEncoder\n",
        "feature_names = preprocessor.get_feature_names_out(categorical_features)\n",
        "selected_features_boruta = feature_names[support_mask]\n",
        "print(\"Boruta selected features:\", list(selected_features_boruta))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HMDLAYd75uph",
      "metadata": {
        "id": "HMDLAYd75uph"
      },
      "source": [
        "For the categorical dependent variable \"lapse.reason\", perform Genetic Algorith feature selection using (from deap) base, creator, tools, algorithms repspectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sY9sH1S95zke",
      "metadata": {
        "id": "sY9sH1S95zke"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "bw6yKyYf8dx6",
      "metadata": {
        "id": "bw6yKyYf8dx6"
      },
      "source": [
        "Optional: Use MLFLow to show you know how to use good development and tracking practices while training the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "prkWG2CWAPOx",
      "metadata": {
        "id": "prkWG2CWAPOx"
      },
      "outputs": [],
      "source": [
        "# Setup MLFlow\n",
        "\n",
        "host =\n",
        "port =\n",
        "mlflow.set_tracking_uri(uri=f\"http://{host}:{port}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "D9G0By67EGZj",
      "metadata": {
        "id": "D9G0By67EGZj"
      },
      "outputs": [],
      "source": [
        "# Model packaging\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sTH_5RwoEHr7",
      "metadata": {
        "id": "sTH_5RwoEHr7"
      },
      "outputs": [],
      "source": [
        "# Model registry\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JhpSOrm4EJiL",
      "metadata": {
        "id": "JhpSOrm4EJiL"
      },
      "outputs": [],
      "source": [
        "# Model serving\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fkLNd6bEKxM",
      "metadata": {
        "id": "8fkLNd6bEKxM"
      },
      "outputs": [],
      "source": [
        "# Model monitoring\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70jdGydFFon-",
      "metadata": {
        "id": "70jdGydFFon-"
      },
      "source": [
        "## Numerical problem, feature selection\n",
        "\n",
        "For the numerical variable \"proportion_risk_premium\", perform a forward and a backward selection on explanatory variables.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "lltiuzAZyXWt",
      "metadata": {
        "id": "lltiuzAZyXWt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "ac93c736-e33c-4b6a-dbc0-2dad19ffcb6d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'scaler_num' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2972445575.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Standardize features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_train_num_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler_num\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Forward selection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m sfs_forward = SequentialFeatureSelector(\n",
            "\u001b[0;31mNameError\u001b[0m: name 'scaler_num' is not defined"
          ]
        }
      ],
      "source": [
        "# Standardize features\n",
        "X_train_num_scaled = scaler_num.fit_transform(X_train_num)\n",
        "\n",
        "# Forward selection\n",
        "sfs_forward = SequentialFeatureSelector(\n",
        "    Lasso(alpha=0.01),\n",
        "    n_features_to_select=10,\n",
        "    direction='forward'\n",
        ")\n",
        "sfs_forward.fit(X_train_num_scaled, y_train_num)\n",
        "print(\"Forward selection features:\", X_train_num.columns[sfs_forward.get_support()])\n",
        "\n",
        "# Backward selection\n",
        "sfs_backward = SequentialFeatureSelector(\n",
        "    Lasso(alpha=0.01),\n",
        "    n_features_to_select=10,\n",
        "    direction='backward'\n",
        ")\n",
        "sfs_backward.fit(X_train_num_scaled, y_train_num)\n",
        "print(\"Backward selection features:\", X_train_num.columns[sfs_backward.get_support()])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BJlJAwZQ6_pO",
      "metadata": {
        "id": "BJlJAwZQ6_pO"
      },
      "source": [
        "For the numerical variable \"proportion_risk_premium\", apply Lasso using LassoCV."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IOfqUffz7ATL",
      "metadata": {
        "id": "IOfqUffz7ATL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "btavk4CC8gY7",
      "metadata": {
        "id": "btavk4CC8gY7"
      },
      "source": [
        "Optional: Use MLFLow to show you know how to use good development and tracking practices while training the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pAlEDL6KETnH",
      "metadata": {
        "id": "pAlEDL6KETnH"
      },
      "outputs": [],
      "source": [
        "# Setup MLFlow\n",
        "\n",
        "host =\n",
        "port =\n",
        "mlflow.set_tracking_uri(uri=f\"http://{host}:{port}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2syae_30ETnH",
      "metadata": {
        "id": "2syae_30ETnH"
      },
      "outputs": [],
      "source": [
        "# Model packaging\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "V7vIRIbPETnI",
      "metadata": {
        "id": "V7vIRIbPETnI"
      },
      "outputs": [],
      "source": [
        "# Model registry\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8iIYeY_QETnI",
      "metadata": {
        "id": "8iIYeY_QETnI"
      },
      "outputs": [],
      "source": [
        "# Model serving\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76Zydou-ETnI",
      "metadata": {
        "id": "76Zydou-ETnI"
      },
      "outputs": [],
      "source": [
        "# Model monitoring\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TDgCQsykrs_5",
      "metadata": {
        "id": "TDgCQsykrs_5"
      },
      "source": [
        "# Explainability of the model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e_UZkcj2rw89",
      "metadata": {
        "id": "e_UZkcj2rw89"
      },
      "source": [
        "## Categorical problem\n",
        "\n",
        "Explain the RandomForest model to a client, that wants to know why their quote is so high by permutation feature selection using permutation_importance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "918Qi51_5reD",
      "metadata": {
        "id": "918Qi51_5reD"
      },
      "outputs": [],
      "source": [
        "result = permutation_importance(clf_cat, X_test_cat, y_test_cat, n_repeats=10, random_state=42)\n",
        "importances = pd.Series(result.importances_mean, index=X_test_cat.columns)\n",
        "print(\"Feature importances:\\n\", importances.sort_values(ascending=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ogt_2Kv9GF6",
      "metadata": {
        "id": "6ogt_2Kv9GF6"
      },
      "source": [
        "Choose another method of your preference to explain the RandomForest model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Jxe13YHE9LCB",
      "metadata": {
        "id": "Jxe13YHE9LCB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "eS87my6hx_3N",
      "metadata": {
        "id": "eS87my6hx_3N"
      },
      "source": [
        "## Numerical problem"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Eh90ACcpr4XT",
      "metadata": {
        "id": "Eh90ACcpr4XT"
      },
      "source": [
        "Explain the Neural Netwok model to a client, that wants to know why their quote is so high using Shapley values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JnoUoZFb5rDv",
      "metadata": {
        "id": "JnoUoZFb5rDv"
      },
      "outputs": [],
      "source": [
        "X_sample = X_test_num_scaled[:100]\n",
        "explainer = shap.KernelExplainer(model.predict, X_train_num_scaled[:100])\n",
        "shap_values = explainer.shap_values(X_sample)\n",
        "\n",
        "shap.summary_plot(shap_values, X_test_num.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CIBT43CY9Rza",
      "metadata": {
        "id": "CIBT43CY9Rza"
      },
      "source": [
        "Choose another method of your preference to explain the NN model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3BMxP6m9Sij",
      "metadata": {
        "id": "a3BMxP6m9Sij"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "aRttCOde6rMA",
      "metadata": {
        "id": "aRttCOde6rMA"
      },
      "source": [
        "# BIAS for the numerical problem\n",
        "\n",
        "After having set a threshold = 0.0002 = 0.02%, define a variable called \"risk_category\" that takes values High or Low if variable \"proportion_risk_premium\" is higher or lower than the treshold.\n",
        "\n",
        "Similarly, define variable \"risk_category_pred\" that does the same but on the basis of the values predicted by the Neural Network model. For these two variables, count the number of High and Low values for Females and Males."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Catz0tFI4SXm",
      "metadata": {
        "id": "Catz0tFI4SXm"
      },
      "outputs": [],
      "source": [
        "# Threshold = 0.0002 = 0.02%\n",
        "threshold = 0.0002\n",
        "\n",
        "df_num['risk_category'] = np.where(df_num['proportion_risk_premium'] > threshold, 'High', 'Low')\n",
        "df_num['risk_category_pred'] ="
      ]
    },
    {
      "cell_type": "markdown",
      "id": "L4N1ibk-gKT_",
      "metadata": {
        "id": "L4N1ibk-gKT_"
      },
      "source": [
        "Using confusion_matrix, compare real and predicted values by counting False positives, False negatives, Trues positives, True negatives."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "S5pT4BN15p5H",
      "metadata": {
        "id": "S5pT4BN15p5H"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "_iXYHCM_HsNY",
      "metadata": {
        "id": "_iXYHCM_HsNY"
      },
      "source": [
        "## Metrics to detect possible bias on predicted values\n",
        "Investigate possible bias on the basis of the metrics:\n",
        "\n",
        "\n",
        "*   Demographic Parity (no bias range: -0.1, 0.1)\n",
        "*   Equal Opportunity (no bias range: -0.1, 0.1)\n",
        "*   Average Odds (no bias range: -0.1, 0.1)\n",
        "*   Disparate Impact (no bias range: 0.8, 1.25)\n",
        "*   Theil Index (no bias range: 0, 0.15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dB_-avSf5nLM",
      "metadata": {
        "id": "dB_-avSf5nLM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "x44KYE-fPC8F",
      "metadata": {
        "id": "x44KYE-fPC8F"
      },
      "source": [
        "## Bias mitigation\n",
        "Proceed with:\n",
        "\n",
        "*   Preprocessing bias mitigation: choose the method, between reweighing and optimized preprocessing, that leads to a greater bias reduction\n",
        "*   In-processing bias mitigation:  choose the method, between Meta Fair\n",
        "*   Postprocessing bias mitigation: Reject option classification and equalized Odds\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "x3S88Rug5n_r",
      "metadata": {
        "id": "x3S88Rug5n_r"
      },
      "outputs": [],
      "source": [
        "# Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9QXy7m2U9gIh",
      "metadata": {
        "id": "9QXy7m2U9gIh"
      },
      "outputs": [],
      "source": [
        "# In-processing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CRxLAFPz9i-x",
      "metadata": {
        "id": "CRxLAFPz9i-x"
      },
      "outputs": [],
      "source": [
        "# Post-Processing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mpjC2V4h9mwp",
      "metadata": {
        "id": "mpjC2V4h9mwp"
      },
      "source": [
        "Compare pros and cons of chosen methods:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46bebee8",
      "metadata": {
        "id": "46bebee8"
      },
      "source": [
        "Write down in text what you think the pros and cons are of the methods above"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "aibe_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}